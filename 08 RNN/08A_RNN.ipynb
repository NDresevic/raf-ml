{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08A_RNN.sol.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "2l-OFjBU_2D6",
        "colab_type": "text"
      },
      "source": [
        "#  RNN (rekurentne neuralne mreže)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jglSyj3pCaKa",
        "colab_type": "text"
      },
      "source": [
        "**\"A class of neural networks with loops in them, allowing information to persist\"**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "OFJr40a4_2D8",
        "colab_type": "text"
      },
      "source": [
        "## Pregled\n",
        "* RNN\n",
        "* Motivacija\n",
        "* Vanila RNN\n",
        "* LSTM\n",
        "* Primeri\n",
        "* Resursi\n",
        "* Implementacija"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX67DRtO1EZU",
        "colab_type": "text"
      },
      "source": [
        "## Rekurentne neuralne mreže (RNN)\n",
        "* Tip NN koji se često koristi kada su ulazni podaci vremenske sekvence tj. kada je umesto **jednog** feature vektora ulaz **sekvenca** feature vektora kroz vreme pri čemu postoje veze između sadašnjosti i prošlosti\n",
        "* **Primeri**: tekst, zvuk, video, genomi, rukopis, berza...\n",
        "* Prva mreža koja nije feedforward, sadrži cikluse\n",
        "* Novi metod treniranja: **backpropagation through time** (BPTT)\n",
        "* Nekoliko fiksnih arhitektura\n",
        "\n",
        "## Motivacija\n",
        "* Nije jasno kako bi standardna NN \"uhvatila\" pravilnosti kroz vreme\n",
        "* Kao primer posmatrajmo problem predikcije naredne reči: za ovo je potreban kontekst, tj. nemoguće je izvršiti predikciju na osnovu jedne prethodne reči\n",
        "\n",
        "## Vanila RNN\n",
        "* Najjednostavniji tip RNN\n",
        "* Prva ilustracija sa [colah:Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/): rekurentna veza (grana = težine tj. FC sloj)\n",
        "* Kako bismo razumeli značenje ove rekurentne veze posmatramo **unrolled** prikaz (druga ilustracija na istom linku) - \"odmotali\" smo mrežu za fiksan broj vremenskih jedinica\n",
        "* Sada stvari deluju jasnije: mreža ima svoje **skriveno stanje** koje zadržava kroz vreme i u svakom koraku ga obogaćuje novim ulazom i daje novi izlaz\n",
        "* To skriveno stanje treba da \"zapamti\" prošlost i iskoristi je za dati zadatak\n",
        "* U opštem slučaju u svakom vremenskom koraku imamo novi ulaz i novi izlaz, ali to ne mora biti slučaj (npr. u klasifikaciji zvuka važan nam je samo poslednji izlaz)\n",
        "* [Još jedna korisna ilustracija koja prikazuje pojedinačne neurone](https://i.imgur.com/yF92R2g.png)\n",
        "* Na novoj ilustraciji imamo označene tri matrice težina: \n",
        "  * $W_{ih}$ - matrica input->hidden, na slici označena kao w1\n",
        "  * $W_{hh}$ - matrica hidden->hidden, na slici označena kao w2\n",
        "  * $W_{ho}$ - matrica hidden->output, na slici označena kao w3\n",
        "* Pored toga ćemo imati dve funkcije aktivacije i dve bias vrednosti:\n",
        "   * $f_h$ - funkcija aktivacije skrivenog sloja (npr. tanh)\n",
        "   * $b_h$ - bias skrivenog sloja\n",
        "   * $f_o$ - funkcija aktivacije izlaznog sloja (npr. sigmoid)\n",
        "   * $b_o$ - bias izlaznog sloja\n",
        "* Kao i do sada ulazni vektor označavamo sa $X$, a izlazni sa $Y$\n",
        "* Skriveno stanje označavamo sa $H$\n",
        "* Sada su kompletne formule za forward propagation **u jednom vremenskom trenutku** kojima od ulaza i starog skrivenog stanja dobijamo izlaz i novo skriveno stanje $(X, H) \\to (H, Y)$:\n",
        "  * $H_{new} = f_h(W_{ih} \\cdot X + W_{hh} \\cdot H + b_h) $\n",
        "  * $Y = f_o(W_{ho} \\cdot H + b_o) $\n",
        "* Što se tiče treninga tj. backpropagation uvodimo metod **backpropagation through time** (BPTT)\n",
        "  * BPTT koristi identične principe kao standardni BP, tj. propagira gradijente unazad od troška i nije suštinski drukčiji - novo ime koristimo jer je više deskriptivno\n",
        "* Postoji i **bidirectional RNN** varijanta u kojoj postoje dva skrivena stanja: jedno se propagira kroz vreme unazad a jedno unapred\n",
        "* **Problem sa vanilla RNN**: ne uspevaju da uhvate pravilnosti kada su sekvence dugačke\n",
        "  - Jedan od konkretnih problema: u slučaju jako dugih sekvenci BPTT stepenuje matricu $W_{hh}$ dovoljno puta da se gradijenti koji služe za ažuriranje težina izgube (vanishing gradients) ili odu u beskonačno (exploding gradients)\n",
        "* Zbog ovoga se umesto \"klasičnih\" RNN koriste naprednije arhitekture\n",
        "* Najpopularnija takva arhitektura je LSTM \n",
        "  \n",
        "## LSTM (Long Short-Term Memory)\n",
        "* Pratiti [ilustracije](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) u sekciji *LSTM Networks*\n",
        "* Umesto jednostavnog pravila za forward propagation koristimo kompleksne **LSTM ćelije**\n",
        "* Jedna ćelija se sadrži od tri kapije: input gate, forget gate, output gate\n",
        "* Skriveno stanje je ujedno i izlaz u nekom momentu ($H_t$)\n",
        "* Pored toga imamo \"stanje ćelije\" ($C_t$)\n",
        "* Jednačine:\n",
        "  * Input gate: $i_t = \\sigma(W_i \\cdot X_t + U_i \\cdot H_{t-1} + b_i)$\n",
        "  * Forget gate: $f_t = \\sigma(W_f \\cdot X_t + U_f \\cdot H_{t-1} + b_f)$\n",
        "  * Output gate: $o_t = \\sigma(W_o \\cdot X_t + U_o \\cdot H_{t-1} + b_o)$\n",
        "  * Dakle svaka kapija je parametrizovana sa dve matrice ($W$ i $U$, pri čemu se to može jednostavno predstaviti kao jedna matrica) i jednom bias vrednošću ($b$)\n",
        "  * Na osnovu trenutnog ulaza ($X_t$) i prethodnog skrivenog stanja ($H_{t-1}$) svaka kapija daje svoju izlaznu vrednost ($i_t$, odnosno $f_t$, odnosno $o_t$)\n",
        "  * Pre računanja novog skrivenog stanja tj. izlaza, moramo ažurirati stanje ćelije\n",
        "  * Za ovo nam je prvo potrebna privremena vrednost zasnovana na novom ulazu koju računamo kao: \n",
        "    * $tmp_t = \\tanh(W_c \\cdot X_t + U_c \\cdot H_{t-1} + b_c)$\n",
        "  * Novo stanje ćelije će biti kombinacija starog stanja ćelije koje \"prolazi\" kroz forget gate i vrednost $tmp_t$ koja \"prolazi\" kroz input gate:\n",
        "    * $C_t = f_t \\circ C_{t-1} + i_t \\circ tmp_t$\n",
        "  * Sada računamo novo skriveno stanje tj. izlaz tako što stanje ćelije \"prolazi\" kroz output gate:\n",
        "    * $H_t = o_t \\circ \\tanh(C_t)$\n",
        "\n",
        "* Pored LSTM često se koristi i GRU (Gated Recurrent Unit)\n",
        "\n",
        "## Primeri\n",
        "* [Modeli za predikciju narednog karaktera - generisanje teksta](http://karpathy.github.io/assets/rnn/charseq.jpeg): Šekspir, Wikipedia, Latex, Linux sors kod\n",
        "* [Još primera korišćenja](https://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd): prepoznavanje rukopisa, generisanje rukopisa, generisanje muzike, prevođenje, image captioning...\n",
        "* Naredni korak: attention?\n",
        "\n",
        "## Resursi\n",
        "- [colah: uvod u RNN fokusiran na LSTM, odlične ilustracije](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "- [karpathy: char-rnn, odličan primer primene](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "- [Implementacija u numpy](https://github.com/rand0musername/psiml2017-workshops/blob/master/4%20-%20RNN/rnn_fwd.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdNxeeJ98sYr",
        "colab_type": "text"
      },
      "source": [
        "## Implementacija RNN (LSTM) u Keras\n",
        "- Radimo sa IMDB sentiment classification datasetom (nad kojim smo radili Naive Bayes u prvom domaćem)\n",
        "- Ovaj skup podataka je previše mali da bi LSTM imao prednost nad jednostavnijim metodama, ali ilustruje Keras sintaksu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osa1u_HlVwde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "0fcae479-efe9-4362-d007-2e63eef33457"
      },
      "source": [
        "# Postoji bug u trenutnoj verziji Keras-a zbog kog nije moguce ucitati imdb set\n",
        "# Jedan od nacina da se ovo privremeno resi je downgrade na verziju numpy 1.16.1\n",
        "!pip install numpy==1.16.1\n",
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/bf/4981bcbee43934f0adb8f764a1e70ab0ee5a448f6505bd04a87a2fda2a8b/numpy-1.16.1-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 4.9MB/s \n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.16.4\n",
            "    Uninstalling numpy-1.16.4:\n",
            "      Successfully uninstalled numpy-1.16.4\n",
            "Successfully installed numpy-1.16.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtyMBv8s8vCx",
        "colab_type": "code",
        "outputId": "87b45ff3-29d6-4e31-b147-30c443c8f2b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "# Koristimo ovoliko najkoriscenijih reci\n",
        "max_features = 20000\n",
        "\n",
        "# Maksimalna duzina neke vremenske sekvence\n",
        "# U nasem slucaju maksimalan broj reci u nekom review-u\n",
        "maxlen = 80\n",
        "\n",
        "# Ostali trening parametri\n",
        "batch_size = 32\n",
        "num_epochs = 15\n",
        "\n",
        "# Ucitavanje podataka\n",
        "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "\n",
        "# Pozivamo funkciju pad_sequences koja pad-uje nase ulazne podatke na duzinu\n",
        "# maxlen tako da svi budu iste duzine\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "# Gradimo model\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding sloj: mapira svaku rec u vektor duzine 128 koji se takodje uci\n",
        "# u toku treninga (https://keras.io/layers/embeddings/)\n",
        "model.add(Embedding(max_features, 128))\n",
        "\n",
        "# LSTM celija sa izlazom velicine 128\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Finalni dense sloj kako bi se dobila jedna vrednost, sa sigmoid aktivacijom\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Standardan binarni crossentropy loss i adam optimizacija\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Trening, 15 epoha\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=num_epochs,\n",
        "          validation_data=(X_test, Y_test))\n",
        "_, acc = model.evaluate(X_test, Y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Accuracy na test skupu:', acc)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "25000\n",
            "(25000, 80)\n",
            "(25000, 80)\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "13952/25000 [===============>..............] - ETA: 1:01 - loss: 0.4934 - acc: 0.7562"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2f48d7021d12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m           validation_data=(X_test, Y_test))\n\u001b[0m\u001b[1;32m     54\u001b[0m _, acc = model.evaluate(X_test, Y_test,\n\u001b[1;32m     55\u001b[0m                             batch_size=batch_size)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}