{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08A_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "2l-OFjBU_2D6",
        "colab_type": "text"
      },
      "source": [
        "#  RNN (rekurentne neuralne mreže)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jglSyj3pCaKa",
        "colab_type": "text"
      },
      "source": [
        "**\"A class of neural networks with loops in them, allowing information to persist\"**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "OFJr40a4_2D8",
        "colab_type": "text"
      },
      "source": [
        "## Pregled\n",
        "* RNN\n",
        "* Motivacija\n",
        "* Vanila RNN\n",
        "* LSTM\n",
        "* Primeri\n",
        "* Resursi\n",
        "* Implementacija"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX67DRtO1EZU",
        "colab_type": "text"
      },
      "source": [
        "## Rekurentne neuralne mreže (RNN)\n",
        "* Tip NN koji se često koristi kada su ulazni podaci vremenske sekvence tj. kada je umesto **jednog** feature vektora ulaz **sekvenca** feature vektora kroz vreme pri čemu postoje veze između sadašnjosti i prošlosti\n",
        "* **Primeri**: tekst, zvuk, video, genomi, rukopis, berza...\n",
        "* Prva mreža koja nije feedforward, sadrži cikluse\n",
        "* Novi metod treniranja: **backpropagation through time** (BPTT)\n",
        "* Nekoliko fiksnih arhitektura\n",
        "\n",
        "## Motivacija\n",
        "* Nije jasno kako bi standardna NN \"uhvatila\" pravilnosti kroz vreme\n",
        "* Kao primer posmatrajmo problem predikcije naredne reči: za ovo je potreban kontekst, tj. nemoguće je izvršiti predikciju na osnovu jedne prethodne reči\n",
        "\n",
        "## Vanila RNN\n",
        "* Najjednostavniji tip RNN\n",
        "* Prva ilustracija sa [colah:Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/): rekurentna veza (grana = težine tj. FC sloj)\n",
        "* Kako bismo razumeli značenje ove rekurentne veze posmatramo **unrolled** prikaz (druga ilustracija na istom linku) - \"odmotali\" smo mrežu za fiksan broj vremenskih jedinica\n",
        "* Sada stvari deluju jasnije: mreža ima svoje **skriveno stanje** koje zadržava kroz vreme i u svakom koraku ga obogaćuje novim ulazom i daje novi izlaz\n",
        "* To skriveno stanje treba da \"zapamti\" prošlost i iskoristi je za dati zadatak\n",
        "* U opštem slučaju u svakom vremenskom koraku imamo novi ulaz i novi izlaz, ali to ne mora biti slučaj (npr. u klasifikaciji zvuka važan nam je samo poslednji izlaz)\n",
        "* [Još jedna korisna ilustracija koja prikazuje pojedinačne neurone](https://i.imgur.com/yF92R2g.png)\n",
        "* Na novoj ilustraciji imamo označene tri matrice težina: \n",
        "  * $W_{ih}$ - matrica input->hidden, na slici označena kao w1\n",
        "  * $W_{hh}$ - matrica hidden->hidden, na slici označena kao w2\n",
        "  * $W_{ho}$ - matrica hidden->output, na slici označena kao w3\n",
        "* Pored toga ćemo imati dve funkcije aktivacije i dve bias vrednosti:\n",
        "   * $f_h$ - funkcija aktivacije skrivenog sloja (npr. tanh)\n",
        "   * $b_h$ - bias skrivenog sloja\n",
        "   * $f_o$ - funkcija aktivacije izlaznog sloja (npr. sigmoid)\n",
        "   * $b_o$ - bias izlaznog sloja\n",
        "* Kao i do sada ulazni vektor označavamo sa $X$, a izlazni sa $Y$\n",
        "* Skriveno stanje označavamo sa $H$\n",
        "* Sada su kompletne formule za forward propagation **u jednom vremenskom trenutku** kojima od ulaza i starog skrivenog stanja dobijamo izlaz i novo skriveno stanje $(X, H) \\to (H, Y)$:\n",
        "  * $H_{new} = f_h(W_{ih} \\cdot X + W_{hh} \\cdot H + b_h) $\n",
        "  * $Y = f_o(W_{ho} \\cdot H + b_o) $\n",
        "* Što se tiče treninga tj. backpropagation uvodimo metod **backpropagation through time** (BPTT)\n",
        "  * BPTT koristi identične principe kao standardni BP, tj. propagira gradijente unazad od troška i nije suštinski drukčiji - novo ime koristimo jer je više deskriptivno\n",
        "* Postoji i **bidirectional RNN** varijanta u kojoj postoje dva skrivena stanja: jedno se propagira kroz vreme unazad a jedno unapred\n",
        "* **Problem sa vanilla RNN**: ne uspevaju da uhvate pravilnosti kada su sekvence dugačke\n",
        "  - Jedan od konkretnih problema: u slučaju jako dugih sekvenci BPTT stepenuje matricu $W_{hh}$ dovoljno puta da se gradijenti koji služe za ažuriranje težina izgube (vanishing gradients) ili odu u beskonačno (exploding gradients)\n",
        "* Zbog ovoga se umesto \"klasičnih\" RNN koriste naprednije arhitekture\n",
        "* Najpopularnija takva arhitektura je LSTM \n",
        "  \n",
        "## LSTM (Long Short-Term Memory)\n",
        "* Pratiti [ilustracije](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) u sekciji *LSTM Networks*\n",
        "* Umesto jednostavnog pravila za forward propagation koristimo kompleksne **LSTM ćelije**\n",
        "* Jedna ćelija se sadrži od tri kapije: input gate, forget gate, output gate\n",
        "* TODO jednačine\n",
        "* Pored LSTM često se koristi i GRU (Gated Recurrent Unit)\n",
        "\n",
        "## Primeri\n",
        "* [Modeli za predikciju narednog karaktera - generisanje teksta](http://karpathy.github.io/assets/rnn/charseq.jpeg): Šekspir, Wikipedia, Latex, Linux sors kod\n",
        "* [Još primera korišćenja](https://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd): prepoznavanje rukopisa, generisanje rukopisa, generisanje muzike, prevođenje, image captioning...\n",
        "* Naredni korak: attention?\n",
        "\n",
        "## Resursi\n",
        "- [colah: uvod u RNN fokusiran na LSTM, odlične ilustracije](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "- [karpathy: char-rnn, odličan primer primene](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "- [Implementacija u numpy](https://github.com/rand0musername/psiml2017-workshops/blob/master/4%20-%20RNN/rnn_fwd.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdNxeeJ98sYr",
        "colab_type": "text"
      },
      "source": [
        "## Implementacija RNN (LSTM) u Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtyMBv8s8vCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}