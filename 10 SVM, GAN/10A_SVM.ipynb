{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10A_SVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "2l-OFjBU_2D6",
        "colab_type": "text"
      },
      "source": [
        "#  Support-vector machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jglSyj3pCaKa",
        "colab_type": "text"
      },
      "source": [
        "**\"A supervised learning technique that pays attention only to the points that are most difficult to tell apart\"**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "OFJr40a4_2D8",
        "colab_type": "text"
      },
      "source": [
        "## Pregled\n",
        "* SVM\n",
        "* TF implementacija"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8-b722UIkW9",
        "colab_type": "text"
      },
      "source": [
        "## SVM\n",
        "- Još jedan supervised learning algoritam za klasifikaciju\n",
        "- Pre neuralnih mreža najkorišćeniji algoritam klasifikacije\n",
        "\n",
        "**Hard margin SVM**\n",
        "- Pretpostavka: tačke su linearno separabilne\n",
        "- Intuicija: fokusiramo se samo na tačke koje je najteže klasifikovati (nasuprot perceptronu koji se fokusira na sve)\n",
        "  - Te tačke su \"support vectors\" i određuju decision boundary\n",
        "  - Dobijamo na neki način \"optimalan\" decision boundary a ne bilo koji\n",
        "- Decision boundary je prava $WX+b=0$, gde je $W$ matrica težina koju želimo da naučimo a $b$ bias\n",
        "- Želimo da za sve tačke u pozitivnoj klasi važi $Wx+b\\ge1$ a za sve tačke u negativnoj $Wx+b\\le-1$\n",
        "  - Large margin\n",
        "- Takođe želimo da prostor između $Wx+b=-1$ i $Wx+b=1$ bude što veći (maksimizujemo $\\frac{2}{||w||}$)\n",
        "- Ovo nas dovodi do problema kvadratne optimizacije, koji može da se reši pretvaranjem u dualni problem (metodom Lagranžovih duala) a zatim gradijentnim spustom (ili samo gradijentnim spustom u originalnoj postavci)\n",
        "- Dobijene težine su nula za većinu tačaka (osim za support vektore)\n",
        "\n",
        "**Soft margin SVM**\n",
        "- Ako tačke nisu linearno separabilne (češći slučaj u praksi) dodajemo \"slack\" promenljive koje olakšavaju naše uslove\n",
        "- Sada $WX+b\\ge1-s_i$ i minimizujemo $\\frac{2}{||w||}+C\\sum{s_i}$\n",
        "\n",
        "**Kernel trick**\n",
        "- SVM je do sada bio linearan model, tj. nalazi linearnu hiperravan koja razdvaja podatke\n",
        "- Kernel trick je metoda kojom možemo dobiti proizvoljno kompleksan decision boundary\n",
        "- Uvodimo funkciju $\\phi$ koja mapira feature prostor u feature prostor veće dimenzije\n",
        "- Sada u tom prostoru linearno razdvojimo podatke\n",
        "  - Što je nelinearan decision boundary u originalnom prostoru\n",
        "- $\\phi(x)$ nikada nemamo eksplicitno ali imamo skalarni proizvod u novom prostoru: $K(x, z) = \\phi(x)^T\\phi(z)$\n",
        "  - Kernel, mera sličnosti dva vektora\n",
        "- Ovo nazivamo \"kernelizacija\" i koristan je trik i za druge machine learning modele (ali se najčešće koristi kod SVM)\n",
        "- Gausov kernel: $K(x, z) = \\exp(-\\frac{||x-z||^2}{2\\sigma^2})$\n",
        "- Validni kerneli, Mercerova teorema\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX67DRtO1EZU",
        "colab_type": "text"
      },
      "source": [
        "## Resursi\n",
        "\n",
        "\n",
        "http://www.svms.org/tutorials/Berwick2003.pdf\n",
        "\n",
        "https://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work\n",
        "\n",
        "http://cs229.stanford.edu/notes/cs229-notes3.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5wdKOmGSq_Q",
        "colab_type": "text"
      },
      "source": [
        "## TF implementacija\n",
        "Hinge loss sa regularizacijom: $ \\frac{1}{2} \\| w \\|_{2}^{2} + C\\sum_{i=1}^{m} \\max \\big(0, 1 - y^{(i)} (w^{T}x^{(i)} + b) \\big)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRrxTzsuO8Up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}