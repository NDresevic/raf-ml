{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06A_Neuralne_mreze.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "2l-OFjBU_2D6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Neuralne mreže"
      ]
    },
    {
      "metadata": {
        "id": "jglSyj3pCaKa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**\"A computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs\"**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "OFJr40a4_2D8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pregled\n",
        "* Neuron\n",
        "* Arhitektura \n",
        "* Forward propagation i backpropagation\n",
        "* Odabir aktivacione funkcije\n",
        "* Klasifikacija neuralnih mreža\n",
        "  * Multilayer Perceptron (MLP)"
      ]
    },
    {
      "metadata": {
        "id": "GX67DRtO1EZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neuralne mreže\n",
        "* Model izračunavanja koji nam omogućava da definišemo kompleksnije hipoteze nego do sad\n",
        "* Jedna istrenirana neuralna mreža = jedna hipoteza\n",
        "  * Istrenirana = pronađene adekvatne vrednosti parametara (kao i do sad)\n",
        "  * I dalje ćemo imati ulaz u vidu feature vektora $x$ \n",
        "  * I dalje ćemo imati izaz u vidu rezultata hipoteze $h_w(x)$\n",
        "* Sastoje se od jednostavnih jedinica - **neurona**\n",
        "\n",
        "## Neuron\n",
        "* Jedan neuron je zapravo trojka $(w, b, f)$ - $w$ je vektor težina, $b$ je *bias* vrednost, a $f$ je **aktivaciona funkcija**\n",
        "* Svaki neuron dobija ulaz (vektor $x$) i transformiše ga po formuli $f(wx+b)$ - linearna kombinacija sa biasom nad kojom je primenjena funkcija aktivacije\n",
        "* 1 neuron sa sigmoidnom aktivacijom je u potpunosti ekvivalentan logističkoj regresiji!\n",
        "* Napomena: Neki autori umesto odvojene vrednosti za bias dodaju još jednu vrednost u vektor težina i na svaki feature vektor dodaju novi feature čija je vrednost uvek jedan - ovo daje iste rezultate ali menja notaciju\n",
        "\n",
        "\n",
        "## Arhitektura\n",
        "* Sada kada znamo kako radi jedan neuron, neuralna mreža je samo kombinacija velikog broja njih\n",
        "* Neuroni se kombinuju \"u dve dimenzije\"\n",
        "* Prvo \"vertikalno\", gde više neurona čini jedan **sloj** mreže (više transformacija odjednom tako da se od feature vektora $x$ dobije novi feature vektor $x'$)\n",
        "  * Kombinujemo parametre neurona u parametre sloja tako da dobijamo $W$ - matricu težina, $b$ - bias vektor, $f$ - jedinstvenu funkciju aktivacije za ceo sloj\n",
        "* Zatim \"horizontalno\", gde se više slojeva niže tako da izlaz jednog postaje ulaz narednog\n",
        "* Ulazni podaci čine **ulazni sloj**, poslednji sloj mreže čini **izlazni sloj**, a svi slojevi između se zovu **skriveni slojevi** (jer njihove vrednosti ne posmatramo direktno)\n",
        "* Dakle, uloga svakog sloja je da transformiše feature vektor u novi feature vektor (osim izlaznog, koji će uglavnom da vrši finalna izračunavanja - npr. softmax)\n",
        "\n",
        "\n",
        "* **Primer**: mreža za binarnu klasifikaciju feature vektora dimenzije 10 sa jednim skrivenim slojem sastavljenim od 3 neurona\n",
        "  * Skicirati mrežu, definisati sve njene parametre, kao i hipotezu\n",
        "  * $a^{(1)} = f^{(1)}(z^{(1)}) = f^{(1)}(W^{(1)}x+b^{(1)})$\n",
        "  * $h = a^{(2)} = f^{(2)}(z^{(2)}) = f^{(2)}(W^{(2)}a^{(1)}+b^{(2)})$\n",
        "  \n",
        "## Forward propagation i backpropagation\n",
        "* Proces izračunavanja kojim mreža dobija hipotezu od ulaznih feature-a naziva se **forward propagation** (propagacija unapred)\n",
        "* Do sada nismo pomenuli kako treniramo mrežu tj. kako tokom treninga ažuriramo parametre\n",
        "* Upravo to radi **backpropagation** (propagacija unazad) - proces koji nakon propagacije unapred računa funkciju troška i propagira njene izvode unazad kroz mrežu, koristeći ih za ažuriranje parametara na svim slojevima\n",
        "  * Detalje ćemo sada preskočiti, ali ovo je ključan mehanizam neuralnih mreža i važno je razumeti kako funkcioniše\n",
        "\n",
        "## Odabir aktivacione funkcije\n",
        "* Pojam aktivacione funkcije smo videli još u primeru logističke regresije (ali tada nismo koristili ovaj naziv)\n",
        "* Svrha aktivacione funkcije je da unese nelinearnost u sistem (bez nje bi mreža sa $N$ slojeva bila ekvivalentna mreži sa jednim slojem, pa ne koristimo punu moć arhitekture)\n",
        "* Kandidati:\n",
        "  * identitet - $f(z) = z$\n",
        "  * sigmoid - $f(z) = \\frac{1}{1+e^{(-z)}}$\n",
        "  * tangens hiperbolički - $f(z) = tanh(z)$\n",
        "  * rectified linear unit (ReLU) - $f(z) = max(0, z)$\n",
        "  * leaky ReLU - $f(z) = z$ ako je $z>=0$, a $f(z)=\\alpha z$ u suprotnom, gde je $\\alpha$ mala unapred određena konstanta"
      ]
    },
    {
      "metadata": {
        "id": "fzlGCIP8zD8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Klasifikacija neuralnih mreža\n",
        "- Neuralne mreže koje nemaju cikluse u grafu povezanosti neurona nazivamo **feedforward mreže** (suprotno, one koje imaju cikluse zovemo **rekurentne mreže**)\n",
        "- Mreže koje smo do sada definisali su isključivo feedforward\n",
        "- Najprostija feedforward mreža je **perceptron**: 0 skrivenih slojeva\n",
        "- Ukoliko imamo 1 ili više skrivenih slojeva takvu mrežu zovemo **multilayer perceptron (MLP)**\n",
        "  - MLP ćemo implementirati za MNIST problem na tri nivoa apstrakcije: \n",
        "    - Prvo, koristeći samo numpy, dakle bez specijalizovanih biblioteka\n",
        "    - Zatim, koristeći TensorFlow\n",
        "    - Zatim, koristeći Keras"
      ]
    },
    {
      "metadata": {
        "id": "xziMMZ6zwo2N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dodatni resursi\n",
        "\n",
        "http://cs231n.github.io/neural-networks-1/\n",
        "\n",
        "\n",
        "http://neuralnetworksanddeeplearning.com/\n",
        "\n",
        "\n",
        "http://karpathy.github.io/neuralnets/\n",
        "\n",
        "\n",
        "http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/"
      ]
    },
    {
      "metadata": {
        "id": "lJIi2volwqos",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementacija MLP uz numpy bez TensorFlow\n",
        "[Link](https://github.com/rand0musername/psiml2017-workshops/blob/master/2%20-%20NN/code/nn.py)"
      ]
    },
    {
      "metadata": {
        "id": "rdNxeeJ98sYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementacija MLP u TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "BtyMBv8s8vCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "19f05536-77a6-4d8d-ba9f-3291c0fec16c"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Ucitavamo MNIST skup podataka.\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "nb_train = len(train_y)\n",
        "nb_test = len(test_y)\n",
        "#print(nb_train, nb_test)\n",
        "#print(train_y)\n",
        "\n",
        "# Prikazujemo prvu sliku.\n",
        "#plt.imshow(train_x[0], cmap='Greys')  \n",
        "\n",
        "train_x = np.reshape(train_x, [nb_train, -1])\n",
        "test_x = np.reshape(test_x, [nb_test, -1])\n",
        "\n",
        "# Parametri mreze\n",
        "learning_rate = 0.001\n",
        "nb_epochs = 16\n",
        "batch_size = 128\n",
        "\n",
        "# Parametri arhitekture\n",
        "nb_input = 28*28    # MNIST data input (img shape: 28*28)\n",
        "nb_hidden1 = 256 # 1st layer number of neurons\n",
        "nb_hidden2 = 256 # 2nd layer number of neurons\n",
        "nb_classes = 10   # MNIST total classes (0-9 digits)\n",
        "\n",
        "# Sama mreza\n",
        "X = tf.placeholder(shape=(None, nb_input), dtype=tf.float32)\n",
        "Y = tf.placeholder(shape=(None), dtype=tf.int32)\n",
        "Y_onehot = tf.one_hot(Y, nb_classes)\n",
        "\n",
        "w = {\n",
        "    '1': tf.Variable(tf.random_normal([nb_input, nb_hidden1])),\n",
        "    '2': tf.Variable(tf.random_normal([nb_hidden1, nb_hidden2])),\n",
        "    'out': tf.Variable(tf.random_normal([nb_hidden2, nb_classes]))\n",
        "}\n",
        "\n",
        "b = {\n",
        "    '1': tf.Variable(tf.random_normal([nb_hidden1])),\n",
        "    '2': tf.Variable(tf.random_normal([nb_hidden2])),\n",
        "    'out': tf.Variable(tf.random_normal([nb_classes]))\n",
        "}\n",
        "\n",
        "f = {\n",
        "    '1': tf.nn.relu,\n",
        "    '2': tf.nn.relu,\n",
        "    'out': tf.nn.softmax\n",
        "}\n",
        "\n",
        "z1 = tf.add(tf.matmul(X, w['1']), b['1'])\n",
        "a1 = f['1'](z1)\n",
        "z2 = tf.add(tf.matmul(a1, w['2']), b['2'])\n",
        "a2 = f['2'](z2)\n",
        "z_out = tf.add(tf.matmul(a2, w['out']), b['out']) # a2 ovde!\n",
        "pred = f['out'](z_out)\n",
        "\n",
        "pred_correct = tf.equal(tf.argmax(pred, 1), tf.argmax(Y_onehot, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(pred_correct, tf.float32))\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=z_out, labels=Y_onehot))\n",
        "\n",
        "# GD je djubre\n",
        "opt_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "#print(train_x.shape)\n",
        "\n",
        "# Trening!\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for epoch in range(nb_epochs):\n",
        "    epoch_loss = 0\n",
        "    nb_batches = int(nb_train / batch_size)\n",
        "    for i in range(nb_batches):\n",
        "      feed = {X: train_x[i*batch_size : (i+1)*batch_size, :], \n",
        "              Y: train_y[i*batch_size : (i+1)*batch_size]}\n",
        "      _, curr_loss = sess.run([opt_op, loss], feed_dict=feed)\n",
        "\n",
        "      epoch_loss += curr_loss\n",
        "\n",
        "    # U svakoj desetoj epohi ispisujemo prosečan loss.\n",
        "    epoch_loss /= nb_train\n",
        "    print('Epoch: {}/{}| Avg loss: {:.5f}'.format(epoch+1, nb_epochs, epoch_loss))\n",
        "    \n",
        "  # Test!\n",
        "  feed = {X: test_x, Y: test_y}\n",
        "  test_acc = sess.run(accuracy, feed_dict = feed)\n",
        "  print('Test acc: {}'.format(test_acc))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/16| Avg loss: 416.39657\n",
            "Epoch: 2/16| Avg loss: 97.68244\n",
            "Epoch: 3/16| Avg loss: 63.09391\n",
            "Epoch: 4/16| Avg loss: 45.20441\n",
            "Epoch: 5/16| Avg loss: 33.77310\n",
            "Epoch: 6/16| Avg loss: 25.81210\n",
            "Epoch: 7/16| Avg loss: 19.98838\n",
            "Epoch: 8/16| Avg loss: 15.65351\n",
            "Epoch: 9/16| Avg loss: 12.04712\n",
            "Epoch: 10/16| Avg loss: 9.36258\n",
            "Epoch: 11/16| Avg loss: 7.29019\n",
            "Epoch: 12/16| Avg loss: 5.85051\n",
            "Epoch: 13/16| Avg loss: 4.40729\n",
            "Epoch: 14/16| Avg loss: 3.58028\n",
            "Epoch: 15/16| Avg loss: 2.85545\n",
            "Epoch: 16/16| Avg loss: 2.18576\n",
            "Test acc: 0.9433000087738037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RTkI0FxN8wIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementacija MLP u biblioteci Keras"
      ]
    },
    {
      "metadata": {
        "id": "z_jz0RLS_euM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "outputId": "55549b17-0c26-4597-8fc1-b335ace06fb1"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "  #tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "  #tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=16)\n",
        "\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "60000/60000 [==============================] - 8s 129us/sample - loss: 0.2253 - acc: 0.9356\n",
            "Epoch 2/16\n",
            "60000/60000 [==============================] - 8s 127us/sample - loss: 0.0921 - acc: 0.9718\n",
            "Epoch 3/16\n",
            "60000/60000 [==============================] - 8s 128us/sample - loss: 0.0609 - acc: 0.9815\n",
            "Epoch 4/16\n",
            " 6208/60000 [==>...........................] - ETA: 6s - loss: 0.0378 - acc: 0.9889"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-957c110bdbe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
