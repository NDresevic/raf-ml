{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06A_Neuralne_mreze.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "2l-OFjBU_2D6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Neuralne mreže"
      ]
    },
    {
      "metadata": {
        "id": "jglSyj3pCaKa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**\"A computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs\"**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "OFJr40a4_2D8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pregled\n",
        "* Neuron\n",
        "* Arhitektura \n",
        "* Forward propagation i backpropagation\n",
        "* Odabir aktivacione funkcije\n",
        "* Klasifikacija neuralnih mreža\n",
        "  * Multilayer Perceptron (MLP)"
      ]
    },
    {
      "metadata": {
        "id": "GX67DRtO1EZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neuralne mreže\n",
        "* Model izračunavanja koji nam omogućava da definišemo kompleksnije hipoteze nego do sad\n",
        "* Jedna istrenirana neuralna mreža = jedna hipoteza\n",
        "  * Istrenirana = pronađene adekvatne vrednosti parametara (kao i do sad)\n",
        "  * I dalje ćemo imati ulaz u vidu feature vektora $x$ \n",
        "  * I dalje ćemo imati izaz u vidu rezultata hipoteze $h_w(x)$\n",
        "* Sastoje se od jednostavnih jedinica - **neurona**\n",
        "\n",
        "## Neuron\n",
        "* Jedan neuron je zapravo trojka $(w, b, f)$ - $w$ je vektor težina, $b$ je *bias* vrednost, a $f$ je **aktivaciona funkcija**\n",
        "* Svaki neuron dobija ulaz (vektor $x$) i transformiše ga po formuli $f(wx+b)$ - linearna kombinacija sa biasom nad kojom je primenjena funkcija aktivacije\n",
        "* 1 neuron sa sigmoidnom aktivacijom je u potpunosti ekvivalentan logističkoj regresiji!\n",
        "* Napomena: Neki autori umesto odvojene vrednosti za bias dodaju još jednu vrednost u vektor težina i na svaki feature vektor dodaju novi feature čija je vrednost uvek jedan - ovo daje iste rezultate ali menja notaciju\n",
        "\n",
        "\n",
        "## Arhitektura\n",
        "* Sada kada znamo kako radi jedan neuron, neuralna mreža je samo kombinacija velikog broja njih\n",
        "* Neuroni se kombinuju \"u dve dimenzije\"\n",
        "* Prvo \"vertikalno\", gde više neurona čini jedan **sloj** mreže (više transformacija odjednom tako da se od feature vektora $x$ dobije novi feature vektor $x'$)\n",
        "  * Kombinujemo parametre neurona u parametre sloja tako da dobijamo $W$ - matricu težina, $b$ - jedinstvenu bias vrednost za ceo sloj, $f$ - jedinstvenu funkciju aktivacije za ceo sloj\n",
        "* Zatim \"horizontalno\", gde se više slojeva niže tako da izlaz jednog postaje ulaz narednog\n",
        "* Ulazni podaci čine **ulazni sloj**, poslednji sloj mreže čini **izlazni sloj**, a svi slojevi između se zovu **skriveni slojevi** (jer njihove vrednosti ne posmatramo direktno)\n",
        "* Dakle, uloga svakog sloja je da transformiše feature vektor u novi feature vektor (osim izlaznog, koji će uglavnom da vrši finalna izračunavanja - npr. softmax)\n",
        "\n",
        "\n",
        "* **Primer**: mreža za binarnu klasifikaciju feature vektora dimenzije 10 sa jednim skrivenim slojem sastavljenim od 3 neurona\n",
        "  * Skicirati mrežu, definisati sve njene parametre, kao i hipotezu\n",
        "  * $a^{(1)} = f^{(1)}(z^{(1)}) = f^{(1)}(W^{(1)}x+b^{(1)})$\n",
        "  * $h = a^{(2)} = f^{(2)}(z^{(2)}) = f^{(2)}(W^{(2)}a^{(1)}+b^{(2)})$\n",
        "  \n",
        "## Forward propagation i backpropagation\n",
        "* Proces izračunavanja kojim mreža dobija hipotezu od ulaznih feature-a naziva se **forward propagation** (propagacija unapred)\n",
        "* Do sada nismo pomenuli kako treniramo mrežu tj. kako tokom treninga ažuriramo parametre\n",
        "* Upravo to radi **backpropagation** (propagacija unazad) - proces koji nakon propagacije unapred računa funkciju troška i propagira njene izvode unazad kroz mrežu, koristeći ih za ažuriranje parametara na svim slojevima\n",
        "  * Detalje ćemo sada preskočiti, ali ovo je ključan mehanizam neuralnih mreža i važno je razumeti kako funkcioniše\n",
        "\n",
        "## Odabir aktivacione funkcije\n",
        "* Pojam aktivacione funkcije smo videli još u primeru logističke regresije (ali tada nismo koristili ovaj naziv)\n",
        "* Svrha aktivacione funkcije je da unese nelinearnost u sistem (bez nje bi mreža sa $N$ slojeva bila ekvivalentna mreži sa jednim slojem, pa ne koristimo punu moć arhitekture)\n",
        "* Kandidati:\n",
        "  * identitet - $f(z) = z$\n",
        "  * sigmoid - $f(z) = \\frac{1}{1+e^{(-z)}}$\n",
        "  * tangens hiperbolički - $f(z) = tanh(z)$\n",
        "  * rectified linear unit (ReLU) - $f(z) = max(0, z)$\n",
        "  * leaky ReLU - $f(z) = z$ ako je $z>=0$, a $f(z)=\\alpha z$ u suprotnom, gde je $\\alpha$ mala unapred određena konstanta"
      ]
    },
    {
      "metadata": {
        "id": "fzlGCIP8zD8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Klasifikacija neuralnih mreža\n",
        "- Neuralne mreže koje nemaju cikluse u grafu povezanosti neurona nazivamo **feedforward mreže** (suprotno, one koje imaju cikluse zovemo **rekurentne mreže**)\n",
        "- Mreže koje smo do sada definisali su isključivo feedforward\n",
        "- Najprostija feedforward mreža je **perceptron**: 0 skrivenih slojeva\n",
        "- Ukoliko imamo 1 ili više skrivenih slojeva takvu mrežu zovemo **multilayer perceptron (MLP)**\n",
        "  - MLP ćemo implementirati za MNIST problem na tri nivoa apstrakcije: \n",
        "    - Prvo, koristeći samo numpy, dakle bez specijalizovanih biblioteka\n",
        "    - Zatim, koristeći TensorFlow\n",
        "    - Zatim, koristeći Keras"
      ]
    },
    {
      "metadata": {
        "id": "xziMMZ6zwo2N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dodatni resursi\n",
        "\n",
        "http://cs231n.github.io/neural-networks-1/\n",
        "\n",
        "\n",
        "http://neuralnetworksanddeeplearning.com/\n",
        "\n",
        "\n",
        "http://karpathy.github.io/neuralnets/\n",
        "\n",
        "\n",
        "http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/"
      ]
    },
    {
      "metadata": {
        "id": "lJIi2volwqos",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementacija MLP uz numpy bez TensorFlow\n",
        "[Link](https://github.com/rand0musername/psiml2017-workshops/blob/master/2%20-%20NN/code/nn.py)"
      ]
    },
    {
      "metadata": {
        "id": "rdNxeeJ98sYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementacija MLP u TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "BtyMBv8s8vCx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RTkI0FxN8wIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementacija MLP u biblioteci Keras"
      ]
    },
    {
      "metadata": {
        "id": "z_jz0RLS_euM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}