{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02A_Linearna_regresija.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "2l-OFjBU_2D6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linearna regresija"
      ]
    },
    {
      "metadata": {
        "id": "jglSyj3pCaKa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**\"Linear approach to modelling the relationship between a scalar response and one or more explanatory variables\"**"
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "OFJr40a4_2D8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pregled\n",
        "* Nadgledano učenje\n",
        "* Prosta linearna regresija\n",
        "* Višestruka linearna regresija: više feature-a\n",
        "* Polinomijalna regresija: polinom umesto linearne funkcije"
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "id": "nk6vDjq9_2EH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Nadgledano učenje\n",
        "\n",
        "**Nadgledano učenje** (induktivno učenje, supervised learning) se bavi problemima u mašinskom učenju gde treba otkriti vezu između ulaznih i izlaznih podataka tj. \"naučiti\" funkciju koja mapira ulaze na izlaze, na osnovu datog skupa parova $(ulaz, izlaz)$. Tu funkciju nazivamo **hipoteza**. \n",
        "\n",
        "Dva glavna tipa problema su **regresija** i **klasifikacija**.\n",
        "\n",
        "[Više o nadgledanom učenju](https://mcerovic.github.io/notes/SupervisedLearning/index.html)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gvmrlF5S_2EN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linearna regresija\n",
        "\n",
        "* **Linearna regresija** (Linear Regression) je jedna od najpoznatijih metoda nadgledanog učenja\n",
        "* Za početak bavićemo se **prostom** linearnom regresijom  (simple LR, univariate LR)\n",
        "  * Imamo jednu ulaznu promenljivu (**feature**) i jednu izlaznu promenljivu\n",
        "* **Notacija**:\n",
        "  * $x$: Ulazni podaci (features), $x^{(i)}$: Ulazni podatak iz i-tog trening primera\n",
        "  * $y$: Izlazni podaci (labels), $y^{(i)}$: Izlazni podatak iz i-tog trening primera\n",
        "  * $m$: Veličina trening skupa - broj semplova\n",
        "* **Hipoteza (model)**: $ h_\\theta(x) = \\theta_0 + \\theta_1 x $\n",
        "  * Linearna funkcija!\n",
        "  * \"Učimo\" parametre $\\theta_0$ i $\\theta_1$, tj. modifikujemo ih tako da budu \"što bolji\"\n",
        "  * Kako znamo kada su \"bolji\"?\n",
        "* **Funkcija troška** (kriterijumska funkcija, funkcija koštanja, cost function, loss function)\n",
        "  * U ovom slučaju za funkciju troška uzimamo **MSE** (srednje kvadratno odstupanje, mean squared error)\n",
        "  * $J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)} - y^{(i)})^2$ (nekada $\\frac{1}{2m}$ zbog lepšeg izvoda)\n",
        " * Dakle, menjamo $\\theta_0$ i $\\theta_1$ tako da **minimizujemo** funkciju troška, kako?\n",
        "* **Optimizacija: gradijentni spust** (gradient descent): iterativni optimizacioni metod za minimizaciju funkcije\n",
        "    * Određuje u kom smeru treba da se krećemo da što efikasnije poboljšamo vrednosti parametara, varijante: \n",
        "        * **Stochastic**: Računamo funkciju troška za svaki trening primer i ažuriramo parametre jednom po primeru\n",
        "        * **Batch**: Agregiramo funkciju troška za sve primere u trening skupu i ažuriramo parametre jednom za ceo skup\n",
        "        * **Minibatch**: Delimo trening skup na manje skupove (*batches*) fiksne veličine, i nad njima agregiramo funkciju troška, jedno ažuriranje parametara po batch-u\n",
        "* [Više o linearnoj regresiji](https://mcerovic.github.io/notes/LinearRegression/index.html)\n",
        "* [Više o gradijentnom spustu](https://mcerovic.github.io/notes/GradientDescent/index.html)\n",
        "* [Više o 3 varijante gradijentnog spusta](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)"
      ]
    },
    {
      "metadata": {
        "id": "95Sfd7ziYXGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Primer: predviđanje očekivanog životnog veka (OŽV) na osnovu nataliteta\n",
        "\n",
        "Istraživanje od strane World Bank:  [World Development Indicators](https://datacatalog.worldbank.org/dataset/world-development-indicators).\n",
        "\n",
        "[Prateća vizualizacija](https://www.google.com/publicdata/explore?ds=d5bncppjof8f9_&ctype=b&strail=false&nselm=s&met_x=sp_dyn_le00_in&scale_x=lin&ind_x=false&met_y=sp_dyn_tfrt_in&scale_y=lin&ind_y=false&met_s=sp_pop_totl&scale_s=lin&ind_s=false&dimp_c=country:region&ifdim=country&iconSize=0.5&uniSize=0.035#!ctype=b&strail=false&bcs=d&nselm=s&met_x=sp_dyn_le00_in&scale_x=lin&ind_x=false&met_y=sp_dyn_tfrt_in&scale_y=lin&ind_y=false&met_s=sp_pop_totl&scale_s=lin&ind_s=false&dimp_c=country:region&ifdim=country&pit=1421395200000&hl=en_US&dl=en_US&ind=false)\n",
        "\n",
        "Ovo ukazuje na određenu korelaciju (veći natalitet => manji životni vek). Pokušaćemo da kvantifikujemo ovaj odnos tj. da naučimo funkciju $OŽV = f(natalitet)$.\n"
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "id": "qthAtzts_2EI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Korak 1: Učitavanje i obrada podataka.\n",
        "# Često ćemo koristiti numpy zbog gotovih funkcija za parsiranje.\n",
        "filename = 'birth_life_2010.tsv'\n",
        "data = dict()\n",
        "data['x'], data['y'] = np.loadtxt(filename, delimiter='\\t', unpack=True, \n",
        "                                  skiprows=1, usecols=(1, 2))\n",
        "# print(len(data['x']), len(data['y']))\n",
        "\n",
        "# Nasumično mešanje podataka: uvek dobra ideja da se eliminiše bilo koja \n",
        "# pravilnost koja potencijalno postoji u ulaznom fajlu.\n",
        "nb_samples = len(data['x'])\n",
        "indices = np.random.permutation(nb_samples)\n",
        "data['x'] = data['x'][indices]\n",
        "data['y'] = data['y'][indices]\n",
        "\n",
        "# Normalizacija: ubrzava gradijentni spust i čini sve promenljive ravnopravnim.\n",
        "# Često dobra ideja, sada ćemo je izostaviti zarad jasnije vizualizacije.\n",
        "# data['x'] = (data['x'] - np.mean(data['x'])) / np.std(data['x'])\n",
        "# data['y'] = (data['y'] - np.mean(data['y'])) / np.std(data['y'])\n",
        "plt.scatter(data['x'], data['y'])\n",
        "plt.xlabel('Natalitet (prosečan broj dece po ženi)')\n",
        "plt.ylabel('Očekivani životni vek')\n",
        "\n",
        "# Korak 2: Model - linearna regresija.\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "theta_0 = tf.Variable(0.0)\n",
        "theta_1 = tf.Variable(0.0)\n",
        "hyp = tf.add(theta_0, tf.multiply(theta_1, X))\n",
        "\n",
        "# Korak 3: Funkcija troška (MSE) i optimizacija.\n",
        "# tf.train.Optimizer je porodica klasa koja generiše op-ove koji nemaju povratnu\n",
        "# vrednost ali pri svakom pokretanju vrše iteraciju algoritma za minimizaciju\n",
        "# određene promenljive.\n",
        "loss = tf.reduce_mean(tf.square(hyp - Y))\n",
        "opt_op = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
        "\n",
        "# Korak 4: Trening.\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  # Izvršavamo 100 epoha treninga (epoha = prolaz kroz sve podatke).\n",
        "  nb_epochs = 100\n",
        "  for epoch in range(nb_epochs):\n",
        "    \n",
        "    # Stochastic Gradient Descent (SGD).\n",
        "    epoch_loss = 0\n",
        "    for sample in range(nb_samples):\n",
        "      feed = {X: data['x'][sample], Y: data['y'][sample]}\n",
        "      _, curr_loss = sess.run([opt_op, loss], feed_dict=feed)\n",
        "      epoch_loss += curr_loss\n",
        "    \n",
        "    # U svakoj desetoj epohi ispisujemo prosečan loss.\n",
        "    epoch_loss /= nb_samples\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print('Epoch: {}/{}| Avg loss: {:.2f}'.format(epoch+1, nb_epochs, \n",
        "                                              epoch_loss))\n",
        "    \n",
        "    # Dodajemo na grafik sve usputne hipoteze da prikažemo napredak.\n",
        "    theta_0_val, theta_1_val = sess.run([theta_0, theta_1])\n",
        "    xs = np.linspace(0, 8, 100)\n",
        "    plt.plot(xs.tolist(), (theta_0_val + theta_1_val * xs).tolist(), \n",
        "            color=(1 - epoch / nb_epochs, epoch / nb_epochs, 0))\n",
        "  \n",
        "  # Ispisujemo i plotujemo finalnu vrednost parametara.\n",
        "  theta_0_val, theta_1_val = sess.run([theta_0, theta_1])\n",
        "  print('theta_0 = {:.2f}, theta_1 = {:.2f}'.format(theta_0_val, theta_1_val))\n",
        "  xs = np.linspace(0, 8, 100)\n",
        "  plt.plot(xs.tolist(), (theta_0_val + theta_1_val * xs).tolist(), color='g')\n",
        "  plt.show()\n",
        "  \n",
        "  # Ispisujemo finalni MSE.\n",
        "  final_loss = sess.run(loss, feed_dict={X: data['x'], Y: data['y']})\n",
        "  print('Finalni loss: {:.5f}'.format(final_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mo4SqOpexQoC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Višestruka linearna regresija\n",
        "\n",
        "* **Višestruka linearna regresija** (Multiple Linear Regression, nekad i Multivariate Linear Regression) je ime za linearnu regresiju gde radimo sa više od jednim ulaznim feature-om\n",
        "* Kao primer radićemo sa datasetom sa sajta **kaggle** pod imenom [**80 cereals**](https://www.kaggle.com/crawford/80-cereals)\n",
        "* Dati su razni podaci o 80 tipova cerealija sa kojima možemo raditi svašta... \n",
        "    * Recimo da želimo da predvidimo promenljivu **rating** na osnovu promenljivih **fat** i **sugars**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* Dodatna notacija: $x_i$ je i-ti feature, dakle $x_i^{(j)}$ je i-ti feature u j-tom trening primeru\n",
        "* Hipoteza je sada: $ h_\\theta(x) = w_0 + w_1 x_1 + ... + w_{n-1} x_{n-1} $\n",
        "  * I dalje linearna funkcija samo po više promenlijvih; umesto fitovanja prave fitujemo ravan tj. hiperravan u određenom broju dimenzija\n",
        "  * \"Učimo\" parametre $w$, $w_0$ (slobodan član) se često naziva *bias*\n",
        "  * **Vektorizovana hipoteza**: $h_\\theta(X)=X\\Theta + bias$, gde je $X$ matrica $m\\times n$ u kojoj je svaki red jedan trening primer, a $\\Theta$ column-vektor dužine $n$\n",
        "* Funkcija troška ostaje ista, kao i metod optimizacije\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* Poći ćemo od prethodnog koda, i koristićemo vektorizovanu formulaciju tako da kod može da radi sa bilo kojim brojem trening primera\n",
        "  * Moramo biti veoma pažljivi sa shape-ovima i broadcastingom\n",
        "  * Pogledajmo primer pre glavnog koda"
      ]
    },
    {
      "metadata": {
        "id": "kpE1-DRBjdr8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Primer za broadcasting"
      ]
    },
    {
      "metadata": {
        "id": "0xoC2z_qfp8J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "axVFpP5vjQK1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 80 cereals višestruka linearna regresija"
      ]
    },
    {
      "metadata": {
        "id": "rcZsdP0J1D0S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tP9jgYg0x4cb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Polinomijalna regresija\n",
        "\n",
        "* **Polinomijalna regresija** (Polynomial Regression) je naziv za regresiju u kojoj umesto linearne funkcije, učimo polinom nekog stepena\n",
        "* Izraženo preko višestruke linearne regresije: od stepenova feature-a pravimo nove feature i onda radimo istu proceduru kao u višestrukoj regresiji\n",
        "* Cela postavka dakle ostaje ista kao u višestrukoj linearnoj regresiji, samo je priprema feature-a drukčija\n",
        "  * $ x_1 = x^1, x_2 = x^2, x_3 = x^3 \\dots $\n",
        "---\n",
        "* Primenimo kod za višestruku regresiju na problem predviđanja **rating** pomoću **sugars** koristeći polinomijalnu regresiju i posmatrajmo kako stepen polinoma utiče na kvalitet rešenja\n",
        "  * Polinomijalna regresija se mogla primeniti i na prvi primer (OŽV) ali su tu podaci po prirodi linearne prirode pa polinomi većeg stepena ne bi doneli značajna poboljšanja"
      ]
    },
    {
      "metadata": {
        "id": "jpARWNk06Ixj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}