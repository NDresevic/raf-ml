{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05B_Naive_Bayes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "2l-OFjBU_2D6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Naive Bayes klasifikator"
      ]
    },
    {
      "metadata": {
        "id": "jglSyj3pCaKa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**\"A probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features\"**"
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "OFJr40a4_2D8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pregled\n",
        "* Generativni modeli\n",
        "* Naive Bayes klasifikator\n",
        "* Primer rada sa tekstom: twitter sentiment analysis\n"
      ]
    },
    {
      "metadata": {
        "id": "EA5xBjrgZuYV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Napomena: formule u nastavku mogu biti matematički gledano blago neprecizne, ali su takve u cilju lakšeg razumevanja suštine*"
      ]
    },
    {
      "metadata": {
        "id": "GX67DRtO1EZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generativni modeli\n",
        "* Klasifikatori se uobičajeno dele na dve glavne grupe: **diskriminativni** i **generativni**\n",
        "* **Diskriminativni modeli** modeluju uslovnu verovatnoću $P(C|X=x)$, tj. verovatnoću da neki ulazni feature ili feature vektor pripada određenoj klasi\n",
        "  * Predikcije iz ovog modela izvlačimo direktno\n",
        "  * Primeri: logistička regresija, k-NN, SVM, neuralne mreže\n",
        "* **Generativni modeli** modeluju združenu distribuciju $P(X, C)$\n",
        "  * Predikcije iz ovog modela izvlačimo kao $P(C|X) = P(X, C) / P(X)$, ali kako je za dato $X$ verovatnoća $P(X)$ fiksna, važi $P(C|X) \\sim P(X, C)$\n",
        "  * Primeri: HMM, Naive Bayes, VAE, GAN"
      ]
    },
    {
      "metadata": {
        "id": "Cle7nN3D1gvH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes klasifikator\n",
        "* Klasifikator koji koristi Bajesovu formulu\n",
        "* **Bajesova formula - podsetnik**: $P(C|X) = \\frac{P(X|C)P(C)}{P(X)} = \\frac{P(X|C)P(C)}{\\sum{P(X|C)P(C)}}$\n",
        "  * $P(C)$ = *prior*\n",
        "  * $P(X|C)$ = *likelihood*, $P(X)$ = *evidence*\n",
        "  * $P(C|X)$ = *posterior*\n",
        "* Kao što je već pomenuto $P(C|X) \\sim P(X|C)P(C)$\n",
        "  * Dakle: \"verovatnoća da feature vektor X pripada klasi C je proporcionalna verovatnoći da bi neki primerak klase C imao feature vektor X, skalirano sa ukupnom verovatnoćom pojavljivanja klase C\"\n",
        "* $P(C)$ lako računamo kao broj pojavljivanja klase C u trening skupu, podeljeno sa veličinom trening skupa\n",
        "  * $P(C) = \\frac{|Elementi~trening~skupa~klase~C|}{|Ceo~trening~skup|}$\n",
        "* Kako izračunati $P(X|C)$? \n",
        "  * Naive Bayes (*naivno*) pretpostavlja *nezavisnost ulaznih promenljivih* , pa je $P(X|C) = P(x_1|C) \\cdot P(x_2|C) \\cdot \\dots \\cdot P(x_N|C)$\n",
        "* Ali kako izračunati $P(x_1|C)$, tj. verovatnoću da bi neki primerak klase $C$ imao prvi feature jednak $x_1$?\n",
        "  * U obradi teksta se često koristi *Multinomial Naive Bayes* sa BoW reprezentacijom koja čuva brojeve pojavljivanja\n",
        "    * Podsetnik: u BoW jedan feature je jedna reč tj. broj pojavljivanja te reči u tekstu\n",
        "  * Sada je dakle $P(x_1|C)$ verovatnoća da neka klasa dokumenata sadrži reč $x_1$, što lako računamo kao frekvenciju ove reči u tekstovima za koje znamo da su iz klase $C$\n",
        "  * $P(Reč_i|Klasa) = \\frac{broj\\_pojavljivanja(Reč_i, Klasa)}{ukupan\\_broj\\_reči(Klasa)}$\n",
        "  * **Nazad na početak**: $P(Klasa|Skup Reči) \\cdot P(Klasa) \\times P(Reč_1 | Klasa) \\times P(Reč_2 | Klasa) \\cdot ... \\cdot P(Reč_N | Klasa)$\n",
        "  * **Predstavljeno preko BoW**: $P(Klasa|BoW vektor) \\sim P(Klasa) \\cdot \\prod{P(Reč_i|Klasa)^{BoW[Reč_i]}}$\n",
        "  * Očigledno, biramo klasu za koju je ova vrednost najveća\n",
        "* **Problem 1**: kada je $N$ veliko množimo/stepenujemo mnogo brojeva od 0 do 1 što loše utiče na preciznost (vrlo lako možemo otići u nulu)\n",
        "  * Rešenje: logaritmujemo sve! \n",
        "  * Sada je $P(Klasa|BoW~vektor) \\sim \\log(P(Klasa)) + \\sum{BoW[Reč_i]\\cdot\\log(P(Reč_i|Klasa))}$\n",
        "* **Problem 2**:  Šta ako je $broj\\_pojavljivanja(Reč_i, Klasa)=0$? Tada $P(Reč_i|Klasa)$ postaje 0 paceo proizvod postaje 0\n",
        "  * Rešenje: **Laplace Smoothing**\n",
        "  * Uvodimo konstantu $\\alpha$ (uglavnom jednaku 1) i smatramo da svaka **klasa** sadrži barem toliko od svake reči\n",
        "  * Dakle, na nivou svake **klase** (ne dokumenta!) dodajemo $\\alpha$ puta svaku reč iz vokabulara\n",
        " * Nova formula: $P(Reč_i|Klasa) = \\frac{broj\\_pojavljivanja(Reč_i, Klasa) + \\alpha}{ukupan\\_broj\\_reči(Klasa) + |Vocab| \\cdot \\alpha}$\n",
        "* [Više o Naive Bayes](http://cs229.stanford.edu/notes/cs229-notes2.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "MUH23_d906GY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes - konačne formule\n",
        "* $P(Klasa|BoW vektor) \\sim P(Klasa) \\cdot \\prod{P(Reč_i|Klasa)^{BoW[Reč_i]}}$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* $P(Klasa) = \\frac{|Elementi~trening~skupa~klase~Klasa|}{|Ceo~ trening~skup|}$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* $P(Reč_i|Klasa) = \\frac{broj\\_pojavljivanja(Reč_i, Klasa) + \\alpha}{ukupan\\_broj\\_reči(Klasa) + |Vocab| \\cdot \\alpha}$"
      ]
    },
    {
      "metadata": {
        "id": "_UOAFS2F1iMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "4dae8b25-1066-4baa-cbd2-4512ab5cc26f"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "  def __init__(self, nb_classes, nb_words, pseudocount):\n",
        "    self.nb_classes = nb_classes\n",
        "    self.nb_words = nb_words\n",
        "    self.pseudocount = pseudocount\n",
        "  \n",
        "  def fit(self, X, Y):\n",
        "    nb_examples = X.shape[0]\n",
        "\n",
        "    # Racunamo P(Klasa) - priors\n",
        "    # np.bincount nam za datu listu vraca broj pojavljivanja svakog celog\n",
        "    # broja u intervalu [0, maksimalni broj u listi]\n",
        "    self.priors = np.bincount(Y) / nb_examples\n",
        "    print('Priors:')\n",
        "    print(self.priors)\n",
        "\n",
        "    # Racunamo broj pojavljivanja svake reci u svakoj klasi\n",
        "    occs = np.zeros((self.nb_classes, self.nb_words))\n",
        "    for i in range(nb_examples):\n",
        "      c = Y[i]\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = X[i][w]\n",
        "        occs[c][w] += cnt\n",
        "    print('Occurences:')\n",
        "    print(occs)\n",
        "    \n",
        "    # Racunamo P(Rec_i|Klasa) - likelihoods\n",
        "    self.like = np.zeros((self.nb_classes, self.nb_words))\n",
        "    for c in range(self.nb_classes):\n",
        "      for w in range(self.nb_words):\n",
        "        up = occs[c][w] + self.pseudocount\n",
        "        down = np.sum(occs[c]) + self.nb_words*self.pseudocount\n",
        "        self.like[c][w] = up / down\n",
        "    print('Likelihoods:')\n",
        "    print(self.like)\n",
        "          \n",
        "  def predict(self, bow):\n",
        "    # Racunamo P(Klasa|bow) za svaku klasu\n",
        "    probs = np.zeros(self.nb_classes)\n",
        "    for c in range(self.nb_classes):\n",
        "      prob = np.log(self.priors[c])\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = bow[w]\n",
        "        prob += cnt * np.log(self.like[c][w])\n",
        "      probs[c] = prob\n",
        "    # Trazimo klasu sa najvecom verovatnocom\n",
        "    print('\\\"Probabilites\\\" for a test BoW (with log):')\n",
        "    print(probs)\n",
        "    prediction = np.argmax(probs)\n",
        "    return prediction\n",
        "  \n",
        "  def predict_multiply(self, bow):\n",
        "    # Racunamo P(Klasa|bow) za svaku klasu\n",
        "    # Mnozimo i stepenujemo kako bismo uporedili rezultate sa slajdovima\n",
        "    probs = np.zeros(self.nb_classes)\n",
        "    for c in range(self.nb_classes):\n",
        "      prob = self.priors[c]\n",
        "      for w in range(self.nb_words):\n",
        "        cnt = bow[w]\n",
        "        prob *= self.like[c][w] ** cnt\n",
        "      probs[c] = prob\n",
        "    # Trazimo klasu sa najvecom verovatnocom\n",
        "    print('\\\"Probabilities\\\" for a test BoW (without log):')\n",
        "    print(probs)\n",
        "    prediction = np.argmax(probs)\n",
        "    return prediction\n",
        "\n",
        "# Primer: https://web.stanford.edu/class/cs124/lec/naivebayes.pdf - slajd 44\n",
        "\n",
        "# Klase: (china, japan)\n",
        "# Vocab: (Chinese, Beijing, Shanghai, Macao, Tokyo, Japan)\n",
        "class_names = ['China', 'Japan']\n",
        "Y = np.asarray([0, 0, 0, 1])\n",
        "X = np.asarray([\n",
        "  [2, 1, 0, 0, 0, 0],\n",
        "  [2, 0, 1, 0, 0, 0],\n",
        "  [1, 0, 0, 1, 0, 0],\n",
        "  [1, 0, 0, 0, 1, 1]\n",
        "])\n",
        "test_bow = np.asarray([3, 0, 0, 0, 1, 1])\n",
        "\n",
        "model = MultinomialNaiveBayes(nb_classes=2, nb_words=6, pseudocount=1)\n",
        "model.fit(X, Y)\n",
        "prediction = model.predict(test_bow)\n",
        "print('Predicted class (with log): ', class_names[prediction])\n",
        "prediction = model.predict_multiply(test_bow)\n",
        "print('Predicted class (without log): ', class_names[prediction])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Priors:\n",
            "[0.75 0.25]\n",
            "Occurences:\n",
            "[[5. 1. 1. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 1. 1.]]\n",
            "Likelihoods:\n",
            "[[0.42857143 0.14285714 0.14285714 0.14285714 0.07142857 0.07142857]\n",
            " [0.22222222 0.11111111 0.11111111 0.11111111 0.22222222 0.22222222]]\n",
            "\"Probabilites\" for a test BoW (with log):\n",
            "[-8.10769031 -8.90668135]\n",
            "Predicted class (with log):  China\n",
            "\"Probabilities\" for a test BoW (without log):\n",
            "[0.00030121 0.00013548]\n",
            "Predicted class (without log):  China\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pltDt4RX2WeR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Primer rada sa tekstom: twitter sentiment analysis\n",
        "*  **Problem**: klasifikovati tvitove na osnovu \"sentimenta\": (neutralan, pozitivan, negativan)\n",
        "*  [Kaggle dataset](https://www.kaggle.com/c/twitter-sentiment-analysis2)\n",
        "*  Potreban ceo proces čišćenja i obrade podataka\n",
        "*  Potrebna feature-izacija (npr. BoW)\n",
        "*  Jedan od mogućih modela može biti Multinomial Naive Bayes\n",
        "*  Potrebna podela skupa podataka na trening/validacioni/test skup radi evaluacije rešenja\n",
        "*  [Primer rešenja](https://github.com/rand0musername/twitter-sentiment)\n",
        "\n"
      ]
    }
  ]
}