{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05B_Naive_Bayes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "2l-OFjBU_2D6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Naive Bayes klasifikator"
      ]
    },
    {
      "metadata": {
        "id": "jglSyj3pCaKa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**\"A probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features\"**"
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "OFJr40a4_2D8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pregled\n",
        "* Generativni modeli\n",
        "* Naive Bayes klasifikator\n",
        "* Primer rada sa tekstom: twitter sentiment analysis na tvitovima\n"
      ]
    },
    {
      "metadata": {
        "id": "GX67DRtO1EZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generativni modeli\n",
        "* Klasifikatori se uobičajeno dele na dve glavne grupe: **diskriminativni** i **generativni**\n",
        "* **Diskriminativni modeli** modeluju uslovnu verovatnoću $P(C|X=x)$, tj. verovatnoću da neki ulazni feature ili feature vektor pripada određenoj klasi\n",
        "  * Predikcije iz ovog modela izvlačimo direktno\n",
        "  * Primeri: logistička regresija, k-NN, SVM, neuralne mreže\n",
        "* **Generativni modeli** modeluju združenu distribuciju $P(X, C)$\n",
        "  * Predikcije iz ovog modela izvlačimo kao $P(C|X) = P(X, C) / P(X)$, ali kako je za dato $X$ verovatnoća $P(X)$ fiksna, važi $P(C|X) \\sim P(X, C)$\n",
        "  * Primeri: HMM, Naive Bayes, VAE, GAN"
      ]
    },
    {
      "metadata": {
        "id": "Cle7nN3D1gvH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes klasifikator\n",
        "* Klasifikator koji koristi Bajesovu formulu\n",
        "* Bajesova formula: $P(C|X) = \\frac{P(X|C)P(C)}{P(X)} = \\frac{P(X|C)P(C)}{\\sum{P(X|C)P(C)}}$\n",
        "  * $P(C)$ = *prior*\n",
        "  * $P(X|C)$ = *likelihood*, $P(X)$ = *evidence*\n",
        "  * $P(C|X)$ = *posterior*\n",
        "* Kao što je već pomenuto $P(C|X) ~ P(X|C)P(C)$\n",
        "  * Dakle: \"verovatnoća da feature vektor X pripada klasi C je proporcionalna verovatnoći da bi neki primerak klase C imao feature vektor X, skalirano sa ukupnom verovatnoćom pojavljivanja klase C\"\n",
        "* $P(C)$ lako računamo kao broj pojavljivanja klase C u trening skupu, podeljeno sa veličinom trening skupa\n",
        "  * $P(Klasa) = \\frac{|Klasa~u~trening~skupu|}{|Trening~skup|}$\n",
        "* Kako izračunati $P(X|C)$? \n",
        "  * Naive Bayes pretpostavlja *nezavisnost ulaznih promenljivih*, pa je $P(X|C) = P(x_1|C) * P(x_2|C) * \\dots * P(x_N|C)$\n",
        "* Ali kako izračunati $P(x_1|C)$, tj. verovatnoću da bi neki primerak klase C imao imao prvi feature jednak $x_1$?\n",
        "  * U obradi teksta se često koristi *Multinomial Naive Bayes* sa BoW reprezentacijom koja čuva brojeve pojavljivanja\n",
        "  * Podsetnik: u BoW jedan feature je jedna reč tj. broj pojavljivanja te reči u tekstu\n",
        "  * Sada je dakle $P(x_1|C)$ verovatnoća da neka klasa dokumenata sadrži reč $x_1$, što lako računamo kao frekvenciju ove reči u tekstovima za koje znamo da su iz klase $C$\n",
        "  * $P(Reč_i|Klasa) = \\frac{broj(Reč_i, Klasa)}{broj(*, Klasa)}$\n",
        "  * **Nazad na početak**: $P(Klasa|Skup Reči) \\sim P(Klasa) * P(Reč_1 | Klasa) * P(Reč_2 | Klasa) * ... * P(Reč_N | Klasa)$\n",
        "  * **Predstavljeno preko BoW**: $P(Klasa|BoW vektor) \\sim P(Klasa) * \\prod{P(Reč_i|Klasa)^{BoW[Reč_i]}}$\n",
        "  * Očigledno, biramo klasu za koju je ova vrednost najveća\n",
        "* **Problem 1**: kada je $N$ veliko množimo/stepenujemo mnogo brojeva od 0 do 1 što loše utiče na preciznost\n",
        "  * Rešenje: logaritmujemo sve! Sada je $P(Klasa|BoW vektor) \\sim \\log(P(Klasa)) + \\sum{BoW[Reč_i]*\\log(P(Reč_i|Klasa))}$\n",
        "* **Problem 2**:  Šta ako je $broj(Klasa)=0$? Deljenje nulom?\n",
        "  * Rešenje: **Laplace Smoothing**\n",
        "  * Uvodimo konstantu $\\alpha$ (uglavnom jednaku 1) i smatramo da svaka klasa sadrži barem toliko od svake reči\n",
        "  * Sada imamo za $\\alpha=1$: $P(Reč_i|Klasa) = \\frac{broj(Reč_i, Klasa) + 1}{broj(*, Klasa) + |Vocab|}$\n",
        "* [Više o Naive Bayes](http://cs229.stanford.edu/notes/cs229-notes2.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "MUH23_d906GY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes - konačne formule\n",
        "* $P(Klasa|BoW vektor) \\sim \\log(P(Klasa)) + \\sum{BoW[Reč_i]*\\log(P(Reč_i|Klasa))}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* $P(Klasa) = \\frac{|Klasa~u~trening~skupu|}{|Trening~skup|}$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* $P(Reč_i|Klasa) = \\frac{broj(Reč_i, Klasa) + \\alpha}{broj(*, Klasa) + |Vocab|\\alpha}$"
      ]
    },
    {
      "metadata": {
        "id": "_UOAFS2F1iMI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pltDt4RX2WeR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Primer rada sa tekstom: twitter sentiment analysis\n",
        "*  Problem: klasifikovati tvitove na osnovu \"sentimenta\": (neutralan, pozitivan, negativan)\n",
        "*  [Kaggle dataset](https://www.kaggle.com/c/twitter-sentiment-analysis2)\n",
        "*  Potreban ceo proces čišćenja i obrade podataka\n",
        "*  Potrebna feature-izacija (npr. BoW)\n",
        "*  Jedan od mogućih modela može biti Multinomial Naive Bayes\n",
        "*  Potrebna podela skupa podataka na trening/validacioni/test skup radi evaluacije rešenja\n",
        "*  [Primer rešenja](https://github.com/rand0musername/twitter-sentiment)\n",
        "\n"
      ]
    }
  ]
}